import archiver from "archiver"
import { createWriteStream } from "fs"
import { readdir, unlink, stat } from "fs/promises"
import path from "path"
import { DEFAULT_DATA_DIR, DEFAULT_BACKUP_DIR, DEFAULT_MAX_BACKUPS } from "@tasktrove/constants"
import { safeReadDataFile } from "@/lib/utils/safe-file-operations"

const BACKUP_DIR = DEFAULT_BACKUP_DIR

function getErrorCode(error: unknown) {
  if (typeof error !== "object" || error === null) {
    return undefined
  }

  const code = Reflect.get(error, "code")
  return typeof code === "string" ? code : undefined
}

function formatError(error: unknown) {
  if (error instanceof Error) {
    const code = getErrorCode(error)
    return code ? `${error.message} (code: ${code})` : error.message
  }

  return String(error)
}

async function ensureBackupDir() {
  try {
    const backupDirStats = await stat(BACKUP_DIR)
    if (!backupDirStats.isDirectory()) {
      throw new Error("Backup path is not a directory.")
    }
  } catch (error) {
    if (getErrorCode(error) === "ENOENT") {
      throw new Error("Backup directory does not exist.")
    }
    throw error
  }
}

async function rotateBackups(maxBackups: number) {
  try {
    // If maxBackups is -1, unlimited backups are allowed
    if (maxBackups === -1) {
      console.log("Unlimited backups configured. No rotation needed.")
      return
    }

    const files = await readdir(BACKUP_DIR)

    // Only manage backup files we created (following our exact naming convention)
    // Pattern: backup-YYYY-MM-DDTHH-MM-SS.zip (auto-generated by this system)
    const autoGeneratedBackupPattern = /^backup-\d{4}-\d{2}-\d{2}T\d{2}-\d{2}-\d{2}\.zip$/

    const backupFiles = files
      .filter((file) => autoGeneratedBackupPattern.test(file))
      .sort((a, b) => {
        // Sort by filename (which includes timestamp) in ascending order
        // Oldest files come first
        return a.localeCompare(b)
      })

    if (backupFiles.length <= maxBackups) {
      console.log(
        `Rotation check: ${backupFiles.length} backups found, less than or equal to max ${maxBackups}. No rotation needed.`,
      )
      return
    }

    console.log(
      `Rotation check: ${backupFiles.length} backups found, exceeding max ${maxBackups}. Starting rotation.`,
    )

    // Get file stats to determine actual sizes (optional for logging)
    const filesWithStats = await Promise.all(
      backupFiles.map(async (file) => ({
        name: file,
        path: path.join(BACKUP_DIR, file),
        stats: await stat(path.join(BACKUP_DIR, file)),
      })),
    )

    // Calculate how many files to delete
    const filesToDelete = filesWithStats.slice(0, backupFiles.length - maxBackups)
    console.log(`Identified ${filesToDelete.length} backups to delete.`)

    // Delete oldest files
    await Promise.all(
      filesToDelete.map(async (file) => {
        await unlink(file.path)
        console.log(`Rotated (deleted) old backup: ${file.name}`)
      }),
    )
  } catch (error) {
    console.error(`Error during backup rotation: ${formatError(error)}`)
  }
}

export async function runBackup() {
  try {
    let maxBackups = DEFAULT_MAX_BACKUPS // Default value

    // Check if auto backup is enabled in settings and get maxBackups value
    try {
      const dataFile = await safeReadDataFile()

      if (dataFile?.settings.data.autoBackup.enabled === false) {
        console.log("Auto backup is disabled in settings. Skipping backup.")
        return
      }

      // Use maxBackups from settings if available
      if (dataFile?.settings.data.autoBackup.maxBackups !== undefined) {
        maxBackups = dataFile.settings.data.autoBackup.maxBackups
      }
    } catch {
      // If data file doesn't exist or can't be read, proceed with backup using default
      console.log("Data file not found or couldn't be read. Proceeding with backup.")
    }

    console.log("Starting daily backup...")
    await ensureBackupDir()

    const timestamp = new Date().toISOString().replace(/[:.]/g, "-").slice(0, -5)
    const backupFileName = `backup-${timestamp}.zip`
    const backupFilePath = path.join(BACKUP_DIR, backupFileName)

    // Use createWriteStream from fs directly
    const output = createWriteStream(backupFilePath)
    const archive = archiver("zip", {
      zlib: { level: 9 }, // Sets the compression level.
    })

    return new Promise<void>((resolve, reject) => {
      output.on("close", async () => {
        console.log(
          `Backup created successfully: ${backupFileName} (${archive.pointer()} total bytes)`,
        )
        try {
          await rotateBackups(maxBackups) // Rotate after successful backup
          resolve()
        } catch (rotationError) {
          console.error(`Error during post-backup rotation: ${formatError(rotationError)}`)
          // Decide if backup failure should depend on rotation failure
          // For now, resolve even if rotation fails, as backup itself succeeded.
          resolve()
        }
      })

      // Handle stream finish event for better completion tracking
      output.on("finish", () => {
        console.log(`Backup file stream finished writing: ${backupFileName}`)
      })

      archive.on("warning", (err: Error & { code?: string }) => {
        if (err.code === "ENOENT") {
          // Log specific warnings but don't necessarily reject
          console.warn(`Archiver warning (ENOENT): ${formatError(err)}`)
        } else {
          // Treat other warnings as potential issues, but maybe not fatal
          console.warn(`Archiver warning: ${formatError(err)}`)
        }
      })

      archive.on("error", (err: Error) => {
        console.error(`Archiver error: ${formatError(err)}`)
        reject(err) // Reject the promise on critical archiver errors
      })

      // Pipe archive data to the file
      archive.pipe(output)

      // Append the entire data directory to the archive
      // The second argument specifies the path prefix inside the zip file (false means root)
      console.log("Archiving data directory.")
      archive.directory(DEFAULT_DATA_DIR, false)

      // Finalize the archive (writes the central directory)
      console.log("Finalizing archive...")
      archive.finalize().catch((err: Error) => {
        // Catch potential errors during finalization
        console.error(`Error during archive finalization: ${formatError(err)}`)
        reject(err)
      })
    })
  } catch (error) {
    console.error(`Failed to run backup: ${formatError(error)}`)
    // Rethrow or handle as appropriate for the scheduler
    throw error
  }
}
