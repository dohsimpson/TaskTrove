import archiver from "archiver"
import { createWriteStream } from "fs"
import { mkdir, readdir, unlink, stat } from "fs/promises"
import path from "path"
import { DEFAULT_DATA_DIR, DEFAULT_BACKUP_DIR, DEFAULT_MAX_BACKUPS } from "@/lib/constants/defaults"
import { safeReadDataFile } from "@/lib/utils/safe-file-operations"

const BACKUP_DIR = DEFAULT_BACKUP_DIR

async function ensureBackupDir() {
  try {
    await mkdir(BACKUP_DIR, { recursive: true })
    console.log("Created backup directory:", BACKUP_DIR)
  } catch (error) {
    console.error("Error creating backup directory:", error)
  }
}

async function rotateBackups(maxBackups: number) {
  try {
    // If maxBackups is -1, unlimited backups are allowed
    if (maxBackups === -1) {
      console.log("Unlimited backups configured. No rotation needed.")
      return
    }

    const files = await readdir(BACKUP_DIR)

    // Only manage backup files we created (following our exact naming convention)
    // Pattern: backup-YYYY-MM-DDTHH-MM-SS.zip (auto-generated by this system)
    const autoGeneratedBackupPattern = /^backup-\d{4}-\d{2}-\d{2}T\d{2}-\d{2}-\d{2}\.zip$/

    const backupFiles = files
      .filter((file) => autoGeneratedBackupPattern.test(file))
      .sort((a, b) => {
        // Sort by filename (which includes timestamp) in ascending order
        // Oldest files come first
        return a.localeCompare(b)
      })

    if (backupFiles.length <= maxBackups) {
      console.log(
        `Rotation check: ${backupFiles.length} backups found, less than or equal to max ${maxBackups}. No rotation needed.`,
      )
      return
    }

    console.log(
      `Rotation check: ${backupFiles.length} backups found, exceeding max ${maxBackups}. Starting rotation.`,
    )

    // Get file stats to determine actual sizes (optional for logging)
    const filesWithStats = await Promise.all(
      backupFiles.map(async (file) => ({
        name: file,
        path: path.join(BACKUP_DIR, file),
        stats: await stat(path.join(BACKUP_DIR, file)),
      })),
    )

    // Calculate how many files to delete
    const filesToDelete = filesWithStats.slice(0, backupFiles.length - maxBackups)
    console.log(`Identified ${filesToDelete.length} backups to delete.`)

    // Delete oldest files
    await Promise.all(
      filesToDelete.map(async (file) => {
        await unlink(file.path)
        console.log(`Rotated (deleted) old backup: ${file.name}`)
      }),
    )
  } catch (error) {
    console.error("Error during backup rotation:", error)
  }
}

export async function runBackup() {
  try {
    let maxBackups = DEFAULT_MAX_BACKUPS // Default value

    // Check if auto backup is enabled in settings and get maxBackups value
    try {
      const dataFile = await safeReadDataFile()

      if (dataFile?.settings.data.autoBackup.enabled === false) {
        console.log("Auto backup is disabled in settings. Skipping backup.")
        return
      }

      // Use maxBackups from settings if available
      if (dataFile?.settings.data.autoBackup.maxBackups !== undefined) {
        maxBackups = dataFile.settings.data.autoBackup.maxBackups
      }
    } catch {
      // If data file doesn't exist or can't be read, proceed with backup using default
      console.log("Data file not found or couldn't be read. Proceeding with backup.")
    }

    console.log("Starting daily backup...")
    await ensureBackupDir()

    const timestamp = new Date().toISOString().replace(/[:.]/g, "-").slice(0, -5)
    const backupFileName = `backup-${timestamp}.zip`
    const backupFilePath = path.join(BACKUP_DIR, backupFileName)

    // Use createWriteStream from fs directly
    const output = createWriteStream(backupFilePath)
    const archive = archiver("zip", {
      zlib: { level: 9 }, // Sets the compression level.
    })

    return new Promise<void>((resolve, reject) => {
      output.on("close", async () => {
        console.log(
          `Backup created successfully: ${backupFileName} (${archive.pointer()} total bytes)`,
        )
        try {
          await rotateBackups(maxBackups) // Rotate after successful backup
          resolve()
        } catch (rotationError) {
          console.error("Error during post-backup rotation:", rotationError)
          // Decide if backup failure should depend on rotation failure
          // For now, resolve even if rotation fails, as backup itself succeeded.
          resolve()
        }
      })

      // Handle stream finish event for better completion tracking
      output.on("finish", () => {
        console.log("Backup file stream finished writing.")
      })

      archive.on("warning", (err: Error & { code?: string }) => {
        if (err.code === "ENOENT") {
          // Log specific warnings but don't necessarily reject
          console.warn("Archiver warning (ENOENT):", err)
        } else {
          // Treat other warnings as potential issues, but maybe not fatal
          console.warn("Archiver warning:", err)
        }
      })

      archive.on("error", (err: Error) => {
        console.error("Archiver error:", err)
        reject(err) // Reject the promise on critical archiver errors
      })

      // Pipe archive data to the file
      archive.pipe(output)

      // Append the entire data directory to the archive
      // The second argument specifies the path prefix inside the zip file (false means root)
      console.log(`Archiving directory: ${DEFAULT_DATA_DIR}`)
      archive.directory(DEFAULT_DATA_DIR, false)

      // Finalize the archive (writes the central directory)
      console.log("Finalizing archive...")
      archive.finalize().catch((err: Error) => {
        // Catch potential errors during finalization
        console.error("Error during archive finalization:", err)
        reject(err)
      })
    })
  } catch (error) {
    console.error("Failed to run backup:", error)
    // Rethrow or handle as appropriate for the scheduler
    throw error
  }
}
